{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bad179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 5s 13ms/step - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6792 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6660 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6511 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6334 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6112 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5857 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.5473 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4940 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.4247 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.31576118]]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikeras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 64\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Define hyperparameters for GridSearchCV\u001b[39;00m\n\u001b[0;32m     60\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m],\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m]\n\u001b[0;32m     63\u001b[0m }\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\n\u001b[0;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m scikeras\u001b[38;5;241m.\u001b[39mKerasClassifier(build_fn\u001b[38;5;241m=\u001b[39mcreate_lstm_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Create a KerasClassifier for GridSearchCV\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scikeras'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sample Twitter dataset and labels (replace these with your actual dataset)\n",
    "tweets = [\"A lot of good things are happening.\", \"We are respected again throughout the world, and that's a great thing. @realDonaldTrump\"]\n",
    "labels = [1, 0]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "# Tokenize the tweets\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences and pad the sequences\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to numpy array\n",
    "y = np.array(labels)\n",
    "\n",
    "# LSTM Model\n",
    "def create_lstm_model(units=100, embedding_dim=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the LSTM model\n",
    "model = create_lstm_model()\n",
    "model.fit(X, y, epochs=10, batch_size=1, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('sentiment_analysis_lstm_model.h5')\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('sentiment_analysis_lstm_model.h5')\n",
    "\n",
    "# Example text for prediction\n",
    "new_text = [\"A lot of good things are happening. We are respected again throughout the world, and that's a great thing. @realDonaldTrump\"]\n",
    "\n",
    "# Tokenize and pad the new text data\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
    "new_X = pad_sequences(new_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict sentiment using the loaded model\n",
    "predicted_sentiment = loaded_model.predict(new_X)\n",
    "\n",
    "# Print the predicted sentiment\n",
    "print(predicted_sentiment)\n",
    "\n",
    "# Define hyperparameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150],\n",
    "    'embedding_dim': [50, 100, 150]\n",
    "}\n",
    "import scikeras\n",
    "\n",
    "model = scikeras.KerasClassifier(build_fn=create_lstm_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "\n",
    "# Create a KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_lstm_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Accuracy: \", grid_result.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3034e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
